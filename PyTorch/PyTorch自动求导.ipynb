{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 标量反向传播\n",
    "> 当目标张量为标量时，backward()无需传入参数。\n",
    "\n",
    "- 例子：假设$w,x,b$都是标量，$z=wx+b$ ，对标量$z$调用backward()方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自动求导的主要步骤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.定义叶子结点，算子节点\n",
    "> 如果需要对Tensor求导，requires_grad要设置为True。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 定义输入张量x\n",
    "x = torch.Tensor([2])\n",
    "# 初始化权重参数w,偏置b,#设置requires_grad为True，使用自动求导\n",
    "w = torch.randn(1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "# 设置前向传播\n",
    "y = torch.mul(w, x)\n",
    "z = torch.add(y, b)\n",
    "# 查看requires_grad属性\n",
    "print(x.requires_grad)\n",
    "print(y.requires_grad)\n",
    "# 因为与w,b具有y依赖关系，所以x,y的requires_grad也是True。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.查看叶子结点，非叶子结点的其他属性\n",
    "- grad_fn:表示梯度函数\n",
    "> 通过运算创建的Tensor(非叶子结点)会自动被赋予grad_fn属性。<br>叶子结点的grad_fn为None。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n",
      "x的grad_fn属性： None\n",
      "y的grad_fn属性： <MulBackward0 object at 0x7fe83935dbb0>\n"
     ]
    }
   ],
   "source": [
    "# 查看非叶子结点y,z的requires_grad属性。\n",
    "print(y.requires_grad)\n",
    "# 查看各节点是不是叶子节点\n",
    "print(x.is_leaf)\n",
    "print(y.is_leaf)\n",
    "# 叶子结点：x,w,b\n",
    "# 非叶子结点：y,z\n",
    "# 查看叶子结点的grad_fn属性\n",
    "print(\"x的grad_fn属性：\", x.grad_fn)\n",
    "# 查看非叶子结点的grad_fn属性\n",
    "print(\"y的grad_fn属性：\", y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.自动求导，实现梯度反向传播\n",
    "> 非叶子节点的梯度调用backward()之后，梯度将被清空。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x的梯度是： None\n",
      "w的梯度是： tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "# 基于z对张量进行反向传播，执行backward之后计算图会清空。\n",
    "z.backward()\n",
    "# 如果需要多次backward()需要设置参数retain_graph为True。此时梯度是累加的。\n",
    "# z.backward(retain_graph=True)\n",
    "\n",
    "# 查看叶子结点的梯度。\n",
    "# 因为x未设置requires_grad属性,默认为False，不求导，所以grad为None。\n",
    "print(\"x的梯度是：\", x.grad)\n",
    "print(\"w的梯度是：\", w.grad)\n",
    "\n",
    "# 查看非叶子结点的梯度\n",
    "# 非叶子节点的梯度调用backward()之后，梯度将被清空。故y,z此时没有梯度。\n",
    "# print(\"y的梯度是：\",y.grad)\n",
    "# print(\"z的梯度是：\",z.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 非标量反向传播\n",
    "> Pytorch只允许标量对张量进行求导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.定义叶子结点，计算结点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 3.]], requires_grad=True)\n",
      "tensor([0., 0.])\n",
      "tensor([[13., 13.]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "# 定义叶子张量x，形状为1x2\n",
    "x = torch.tensor([[2, 3]], dtype=torch.float, requires_grad=True)\n",
    "print(x)\n",
    "# 初始化雅可比矩阵\n",
    "J = torch.zeros(2, 2)\n",
    "print(J[0])\n",
    "# 初始化目标张量，形状为1x2\n",
    "y = torch.zeros(1, 2)\n",
    "# 定义y与x之间的映射关系\n",
    "# y1 = x1**2+3*x2\n",
    "# y2 = x2**2+2*x1\n",
    "y[0, 0] = x[0, 0]**2+3*x[0, 1]\n",
    "y[0, 1] = x[0, 1]**2+2*x[0, 0]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.调用backward()获取y对x的梯度\n",
    "> 需要重复使用backward()时,retain_graph=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 3.],\n",
      "        [2., 6.]])\n"
     ]
    }
   ],
   "source": [
    "# 生成y1对x的梯度\n",
    "y.backward(torch.Tensor([[1, 0]]), retain_graph=True)\n",
    "J[0] = x.grad\n",
    "# 因为梯度是累加的，所以需要清除对x的梯度\n",
    "x.grad = torch.zeros_like(x.grad)\n",
    "# 生成y2对x的梯度\n",
    "y.backward(torch.Tensor([[0, 1]]))\n",
    "J[1] = x.grad\n",
    "# 雅可比矩阵的值\n",
    "print(J)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
